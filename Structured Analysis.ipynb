{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step analysis of Ant Colony Simulations\n",
    "\n",
    "### Ant Colony Evaluation through Simulations\n",
    "\n",
    "In the last half of the century there were created techniques to analyze behavior through random repetition.\n",
    "These techniques are known as Monte Carlo Methods, this is name came from the old name of the casino game Roullete.\n",
    "Papers were written showing the power of simulating an experiment over thousand and thousand times.\n",
    "\n",
    "So with this idea, I created an script that runs the Ant Colony System to solve the graph of 14 Cities in Burma.\n",
    "And based on this script and the Analysis of the results I also created an Step by Step Analysis to later confirm\n",
    "the results found to this instance in other graphs.\n",
    "\n",
    "Ant Colony System is based on 6 Parameters: Alpha, Beta, Rho, Elite, Ant and Limit\n",
    "\n",
    "1) Alpha is the parameter related to the pheromone amount in an edge\n",
    "2) Beta is the parameter related to the shortness of an edge\n",
    "3) Rho is the parameter related to the pheromone decrease rate\n",
    "4) Elite is the parameter related to the pheromone boost to the best ant of one generation\n",
    "5) Ant is the parameter related to the amount of Ants per generation\n",
    "6) Limit is the parameter related to the number of generations the algorithm is going to run\n",
    "\n",
    "To each of this parameter were chosen a sample of values to iterate over, and since the algorithm relies on randomness\n",
    "every group of the parameters were repeated 30 times, this way ensuring that a reliable solution may come appear.\n",
    "\n",
    "1) Initialize with the first value\n",
    "2) For 30 Times do\n",
    "  1) Run ACS with chosen parameters\n",
    "  2) Save sequence and results\n",
    "3) Change one parameter\n",
    "4) Repeat 2 until all parameters are tested\n",
    "\n",
    "This Process generated a dataset with over a Million data points and with this Dataset I going to go step by step in finding the best values to find a good solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step\n",
    "to read and clean the data turn it in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing needed libraries\n",
    "# DataFrame library\n",
    "import pandas as pd\n",
    "# numerical library\n",
    "import numpy as np\n",
    "# Scientifical library\n",
    "import scipy as sp\n",
    "# Statistical Library with R notation for Linear Models\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Creating the pandas DataFrame from csv file\n",
    "df = pd.read_csv(\"burma14.csv\")\n",
    "#cleaning cells with Not Available data, or erased cell\n",
    "clean = df.dropna()\n",
    "#Creating a column with exceeding distance from optimal result\n",
    "clean.loc[:,'delta'] = clean.loc[:,'distance'] - 3323\n",
    "#Creating column with exceeding percentage from optimal result\n",
    "clean.loc[:,'%delta'] = clean.loc[:,'delta']/3323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Step\n",
    "**create hexagram of the columns**\n",
    "\n",
    "Answering the question: *How many times each value of a parameter got to optimal result*\n",
    "\n",
    "With a hexagram we answer the general question: \"How many times each value y was found by each value x\"\n",
    "and this is displayed by intensity of color, a colorbar is displayed by side to help analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterate through each parameter column\n",
    "for name in clean.columns:\n",
    "    # since we are interested in the relation between distance and another parameter, those relative columns\n",
    "    # are excluded from analysis\n",
    "    if name in ['distance','ID','delta','%delta'] :\n",
    "        continue\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    # create plot type and change setup\n",
    "    d = ax.hexbin(clean[name],clean['distance'], cmap=plt.cm.Blues, alpha=0.8)\n",
    "    plt.xlabel(name,fontsize=14)\n",
    "    plt.ylabel('Distance in Km',fontsize=14)\n",
    "    plt.title('Hexbin of times distance X was reached with parameter '+name,fontsize=14)\n",
    "    cb = fig.colorbar(d)\n",
    "    cb.set_label('number of times',fontsize=14)\n",
    "    fig.savefig('img/plot_hexbin_'+name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Step - Refining\n",
    "\n",
    "**Repeat hexagrams**\n",
    "\n",
    "After analizing the result, we can see a lot of good results already, but the final answer is still obscure\n",
    "since the results are similar to a few values.\n",
    "\n",
    "To reduce it we are increasing the **minimum** value, this way only after the count reach this minimum we display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in clean.columns:\n",
    "    if name in ['distance','ID','delta','%delta'] :\n",
    "        continue\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    \n",
    "    # change point\n",
    "    # before we only plotted the hexagram, without tuning\n",
    "    # this time we set the minimal count parameter, so we only display values after it\n",
    "    # we set it different based on the results\n",
    "    if name == 'alpha':\n",
    "        d = ax.hexbin(clean[name],clean['distance'],mincnt=8000, cmap=plt.cm.Blues)\n",
    "    elif name == 'rho':\n",
    "        d = ax.hexbin(clean[name],clean['distance'],mincnt=25000, cmap=plt.cm.Blues)\n",
    "    elif name == 'limit':\n",
    "        d = ax.hexbin(clean[name],clean['distance'],mincnt=48000, cmap=plt.cm.Blues)\n",
    "    else:\n",
    "        d = ax.hexbin(clean[name],clean['distance'],mincnt=18000, cmap=plt.cm.Blues)\n",
    "        \n",
    "    plt.xlabel(name,fontsize=14)\n",
    "    plt.ylabel('Distance in Km',fontsize=14)\n",
    "    plt.title('Hexbin of times distance X was reached with parameter '+name,fontsize=14)\n",
    "    cb = fig.colorbar(d)\n",
    "    cb.set_label('number of times',fontsize=14)\n",
    "    fig.savefig('img/plot_hexbin_refmin_'+name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Unfortunatelly the dataset is not uniformilly shapped because of initial try&error tentatives of creating a standalone script to simulate it.\n",
    "\n",
    "Even so, every case has at least 30 simulations of it, initial cases have a few more.\n",
    "Because of it, we can say that the points with most repeated solution are the best ones.\n",
    "\n",
    "From the Graphs would be:\n",
    "* Ant: 10 ~ 50 (Ant 50 is not fully simulated)\n",
    "* Alpha: 7\n",
    "* Beta: 7\n",
    "* Rho: 0.2, 0.5, 0.8\n",
    "* Elite: 0.6\n",
    "* Limit: 100\n",
    "\n",
    "Next step is to verify this with statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Step \n",
    "**Statistical Analysis**\n",
    "\n",
    "The idea in the previous steps were to gain information about this phenomenon, what is happening, is it replicable, when does it happens, how does it happens. Those kind of question were answered, at least in part. Now remains the question, **_why_**?\n",
    "\n",
    "And now we try to answer it. With some statistical tools we try to gain knowledge about this phenomenon\n",
    "\n",
    "1) Linear Model 1 variable\n",
    "Our first try is to see how every parameter answer as a LM, we seek something like\n",
    "\n",
    "```\n",
    "y = a*x + b\n",
    "```\n",
    "This code show as y as variable dependable of x by a scale of a with an error of b\n",
    "\n",
    "We are intereted in 3 values: \n",
    "* The parameter value it self, in our exemple the value a\n",
    "* The P-value, it shows the statistical significance of our parameter, in simple words: 'Is my value random?'\n",
    "* The R^2 value, it shows how much this LM can explain my data\n",
    "* The Correlation between our parameter and the distance, if the parameter increase 1 point what happens to the distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# auxiliar array\n",
    "aux = []\n",
    "# same trick to ignore ours parameters\n",
    "for name in clean.columns:\n",
    "    if name in ['distance','ID','delta','%delta'] :\n",
    "        continue\n",
    "    \n",
    "    # formula for the linear model\n",
    "    # this notation can be read as \n",
    "    # distance is proportional to parameter name\n",
    "    # we are interested in find this proportion\n",
    "    formula = \"Q('distance') ~ Q('\"+name+\"')\"\n",
    "    model = smf.ols(formula,data=clean)\n",
    "    # if the Number of OBServations is lower than ha\n",
    "    if model.nobs < len(clean)/2:\n",
    "        continue\n",
    "        \n",
    "    results = model.fit()\n",
    "    \n",
    "    #insert R^2 values in aux array with the parameter name\n",
    "    aux.append((results.rsquared, name))\n",
    "    #The Real Values of the Parameters\n",
    "    print(results.params)\n",
    "    #The P-value between 0 and 1, 0 is better\n",
    "    print(results.pvalues)\n",
    "    print(' ')\n",
    "    #Pearson Correlation - Correlation\n",
    "    print('pearson correlation distance and '+name,clean[name].corr(clean['distance'],method='pearson'))\n",
    "    \n",
    "    # Spearman Correlation - Correlation - with more robust approach to non linearity\n",
    "    print('spearman correlation distance and '+name,clean[name].corr(clean['distance'],method='spearman'))\n",
    "    print(' ')\n",
    "\n",
    "    \n",
    "#rank the array from greatest to lowest\n",
    "aux.sort(reverse=True)\n",
    "for mse,name in t:\n",
    "    print(name,mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_First Analysis_**\n",
    "\n",
    "A first look through this numbers show us that Beta is the most explanatory data, with a _85%_ correlation with the decrease of distance. But our linear model only shows a R^2 of 63%\n",
    "\n",
    "This may be related to seeing beta as an Linear parameter rather than an Non-Linear and even a Categorical one.\n",
    "\n",
    "Looking at the graphs may helps us better understand it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in clean.columns:\n",
    "    if name in ['distance','ID','delta','%delta'] :\n",
    "        continue\n",
    "\n",
    "    formula = \"Q('distance') ~ Q('\"+name+\"')\"\n",
    "    formula1 = \"Q('%delta') ~ Q('\"+name+\"')\"\n",
    "    model = smf.ols(formula,data=clean)\n",
    "    model1 = smf.ols(formulaq,data=clean)\n",
    "    if model.nobs < len(clean)/2:\n",
    "        continue\n",
    "        \n",
    "    results = model.fit()\n",
    "    results1 = model1.fit()\n",
    "\n",
    "    #ploting a figure with our data points and with the modelled points\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.plot(clean[name], clean['distance'], 'o', label=\"Data\")\n",
    "    ax.plot(clean[name], results.fittedvalues, '^r', label=\"Predicted\")\n",
    "    plt.xlabel(name,fontsize=14)\n",
    "    plt.ylabel('Distance in Km',fontsize=14)\n",
    "    plt.title('Variation of Final Distance by variation of '+name,fontsize=14)\n",
    "    plt.grid(True)\n",
    "    legend = ax.legend(loc=\"best\")\n",
    "    fig.savefig('img/plot_distance_'+name+'.png')\n",
    "    #ploting a figure with our data points and with the modelled points\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.plot(clean[name], clean['%delta'], 'o', label=\"Data\")\n",
    "    ax.plot(clean[name], results1.fittedvalues, '^r', label=\"Predicted\")\n",
    "    plt.xlabel(name,fontsize=14)\n",
    "    plt.ylabel('Delta in %',fontsize=14)\n",
    "    plt.title('% of exceed distance by variation of '+name,fontsize=14)\n",
    "    plt.grid(True)\n",
    "    legend = ax.legend(loc=\"best\")\n",
    "    fig.savefig('img/plot_delta100_'+name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both graphs for each parameter are essentially the same. The first one shows the Distance ~ Paramter and the second one %Delta ~ Parameter. The latter says about how much extra distance from the optimal this parameter create, or how close to optimal does it get?\n",
    "\n",
    "\n",
    "We can see that because of the way this simulations were designed. With the Economics idea of Ceteris Paribus in mind, each simulation only changes one value and sees how the whole system reacts to it. This way even with a non-linear moviment as the Beta parameter, we should still treaty each possible value as category with a binary way, either on or off.\n",
    "\n",
    "So in our refing step we change from LM to Categorical Binary Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# auxiliar array\n",
    "aux = []\n",
    "# same trick to ignore ours parameters\n",
    "for name in clean.columns:\n",
    "    if name in ['distance','ID','delta','%delta'] :\n",
    "        continue\n",
    "    \n",
    "    # formula for the linear model\n",
    "    # this notation can be read as \n",
    "    # distance is proportional to parameter name\n",
    "    # we are interested in find this proportion\n",
    "    formula = \"Q('distance') ~ C(Q('\"+name+\"'))-1\"\n",
    "    model = smf.ols(formula,data=clean)\n",
    "    # if the Number of OBServations is lower than ha\n",
    "    if model.nobs < len(clean)/2:\n",
    "        continue\n",
    "        \n",
    "    results = model.fit()\n",
    "    \n",
    "    #insert R^2 values in aux array with the parameter name\n",
    "    aux.append((results.rsquared, name))\n",
    "    #The Real Values of the Parameters\n",
    "    print(results.params)\n",
    "    #The P-value between 0 and 1, 0 is better\n",
    "    print(results.pvalues)\n",
    "    print(' ')\n",
    "    #Pearson Correlation - Correlation\n",
    "    print('pearson correlation distance and '+name,clean[name].corr(clean['distance'],method='pearson'))\n",
    "    \n",
    "    # Spearman Correlation - Correlation - with more robust approach to non linearity\n",
    "    print('spearman correlation distance and '+name,clean[name].corr(clean['distance'],method='spearman'))\n",
    "    print(' ')\n",
    "\n",
    "    \n",
    "#rank the array from greatest to lowest\n",
    "aux.sort(reverse=True)\n",
    "for mse,name in t:\n",
    "    print(name,mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our results starts to improve, even values with statistical insignificance, given from high P-values, are better indicating their differences and also sorting which is the best of them.\n",
    "\n",
    "Our R^2 is closer to our correlation now. This confirms that this Model better explain what our simulations intended to show, which parameter changes more our result, and from a preset of values which gives better performance.\n",
    "\n",
    "From our Analysis until now they are:\n",
    "\n",
    "1. Beta\n",
    "2. Limit\n",
    "3. Ant\n",
    "4. Alpha\n",
    "5. Elite - not significant\n",
    "6. Rho - not significant\n",
    "\n",
    "Our next step is to combine Beta and this other results to see what that bringes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating arrays for tuples of data\n",
    "betas = np.array((0,0.25,0.5,0.75,1,1.5,2,2.5,3,3.5,4,4.5,5,6,7))\n",
    "alphas = np.array((0,0.25,0.5,0.75,1,1.5,2,2.5,3,3.5,4,4.5,5,6,7))\n",
    "ants = np.array((10,50))\n",
    "limits = np.array((10,100))\n",
    "rhos = np.array((0.2,0.4,0.5,0.6,0.8))\n",
    "elites = np.array((0,0.2,0.4,0.5,0.6,0.8))\n",
    "\n",
    "alpha = [(beta,i) for beta in betas for i in alphas]\n",
    "ant = [(beta,i) for beta in betas for i in ants]\n",
    "limit = [(beta,i) for beta in betas for i in limits]\n",
    "rho = [(beta,i) for beta in betas for i in rhos]\n",
    "elite = [(beta,i) for beta in betas for i in elites]\n",
    "\n",
    "\n",
    "\n",
    "#Dict of tuples with the parameters values\n",
    "dfs = {'alpha':alpha,'ant':ant,'limit':limit,'rho':rho,'elite':elite}\n",
    "\n",
    "#skip trick\n",
    "for name in clean.columns:\n",
    "    if name in ['distance','ID','beta','delta','%delta'] :\n",
    "        continue\n",
    "    \n",
    "    #formula for Two Categories\n",
    "    formula = \"Q('distance') ~ C(Q('beta')) + C(Q('\"+name+\"')) -1\"\n",
    "    model = smf.ols(formula,data=clean)     \n",
    "    results = model.fit()\n",
    "    \n",
    "    #plot data\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.plot(clean[name], clean['distance'], 'o', label=\"Data\")\n",
    "    #generate a data frame from the tuples\n",
    "    df = pd.DataFrame(dfs[name], columns=['beta',name])\n",
    "    #separate the different Betas so two show with diffrent colors\n",
    "    for i in betas:\n",
    "        x = df.loc[df.loc[:,'beta']==i]\n",
    "        ax.plot(x[name],results.predict(x) , '^-', label=\"Predicted beta \"+str(i))\n",
    "    plt.xlabel(name,fontsize=14)\n",
    "    plt.ylabel('Distance in Km',fontsize=14)\n",
    "    plt.title('Variation of  Distance by variation of '+name+'with Beta separation',fontsize=14)\n",
    "    plt.grid(True)\n",
    "    legend = ax.legend(loc=\"upper right\",bbox_to_anchor=(1.3, 1.0))\n",
    "    fig.savefig('img/plot_'+name+'_beta.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Analysis**\n",
    "\n",
    "Confirmation! So, the graphs confirm what we already knew. A higher Beta leads to a lower Distance, in all case-scenarios. This confirms the Spearman and Pearson correlation showing that beta alone explain most of the model.\n",
    "So we shall pick Beta 7. \n",
    "These graph also show that Rho and Elite have now real change between different cases. This way, there is no real picking, from this analysis alone.\n",
    "\n",
    "Alpha in the other hand shows two moments of slight increases in 0 to 0.25 and 4 to 4.5, and no indication of decrease. So alpha 0 is the best result.\n",
    "\n",
    "In contrast, Ant and Limit have great descent in the cases studied. Both of them shows that higher values lead to lower distance. From our cases, Ant 50 and Limit 100 are the choosen ones.\n",
    "\n",
    "Remembering:\n",
    "* Alpha: 0\n",
    "* Beta: 7 (Higher)\n",
    "* Rho: Anyone\n",
    "* Elite: Anyone\n",
    "* Ant: 50 (Higher)\n",
    "* Limit: 100 (Higher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi Category Model Analysis**\n",
    "\n",
    "One last is to test to do is a stacked test. Change one item at a time we going to construct Seven Models and see how including that item change the previous model or the base model.\n",
    "\n",
    "As result from this we can see how each value possible for the parameters affects our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seven Model based on all values possible for each category without an intercept\n",
    "formula1 = 'distance ~ C(beta)-1'\n",
    "formula2 = 'distance ~ C(beta)+C(ant)-1'\n",
    "formula3 = 'distance ~ C(beta)+C(limit)-1'\n",
    "formula4 = 'distance ~ C(beta)+C(limit)+C(ant)-1'\n",
    "formula5 = 'distance ~ C(beta)+C(limit)+C(ant)+C(alpha)-1'\n",
    "formula6 = 'distance ~ C(beta)+C(limit)+C(ant)+C(rho)-1'\n",
    "formula7 = 'distance ~ C(beta)+C(limit)+C(ant)+C(elite)-1'\n",
    "\n",
    "formulas = [formula1,formula2,formula3,formula4,formula5,formula6,formula7]\n",
    "t = []\n",
    "models = {}\n",
    "\n",
    "for formula in formulas:\n",
    "    model = smf.ols(formula,data=clean)\n",
    "    results = model.fit()\n",
    "        \n",
    "    t.append((results.rsquared, formula))\n",
    "    models[formula] = results\n",
    "    \n",
    "t.sort(reverse=True)\n",
    "for mse,formula in t:\n",
    "    #difference between this model and the Base model\n",
    "    diff = mse - t[6][0]\n",
    "\n",
    "    print(formula,mse*100,diff*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categories Analysis**\n",
    "\n",
    "Our latest test shows that Parameter Beta explain most of the Model, but Ant and Limit also contribute signifcantly. In opposite direction, Rho, Limit and Alpha impact very little with an explanation of 0,003% more than the other had already explained.\n",
    "\n",
    "The values shown for the parameters are also in the way we expected, except for _alpha_. Even with low influence in the overall Model, the formula result indicates that Alpha 4.5 as the resposible for most impact in lowering the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conjugate Analysis and going Further**\n",
    "\n",
    "Both our analysis by stats and repeated solution shows similarities in Beta and Limit. The Parameter Ant tends to higher values, but no concrete answer since it was not fully simulated by the end of this notebook with the repeated process, but least stick with this idea.\n",
    "\n",
    "There were also some differences. Alpha in the stats should be 0, but there were more lower values in with alpha 7. Rho and Elite also have preferentiable values with the repeated process, but stats say there is no significance in them, so anyone is a good choice.\n",
    "\n",
    "To sort this out, I ran a new simulation with values:\n",
    "* Limit: 100, 500, 1000\n",
    "* Ant: 50, 100, 200\n",
    "* Alpha: 0, 4.5, 7\n",
    "* Beta: 7, 8, 9\n",
    "* Rho: 0.8\n",
    "* Elite: 0.6\n",
    "\n",
    "This new simulation is going to test the differences we found and our new beliefs:\n",
    "- Which Alpha is the best?\n",
    "- The Higher value for Limit, Ant and Beta are really better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
